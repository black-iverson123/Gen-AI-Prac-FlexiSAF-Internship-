{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all required libraries and modules, initializing cuda for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing the Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epochs = 10\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "seq_length = 20\n",
    "lr = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(StoryModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batchs_size):\n",
    "        hidden = (torch.zeros(num_layers, batchs_size, hidden_size).to(device),\n",
    "                  torch.zeros(num_layers, batchs_size,hidden_size).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation: \n",
    "Here i use the gutenberg text file from nltk library \"Alice in Wonderland\" by lewis caroll\n",
    "\n",
    "note print statements and values given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = [gutenberg.raw(\"carroll-alice.txt\"),\n",
    "        gutenberg.raw(\"shakespeare-hamlet.txt\"),\n",
    "        ]\n",
    "\n",
    "# confirm file is loaded\n",
    "assert stories is not None, \"files not found\"\n",
    "\n",
    "#combining stories to form one text corpus seperated by token\n",
    "combined_stories = \"<sep>\".join(stories)\n",
    "\n",
    "# all words will now be converted to lower forms and split i.e tokenized\n",
    "#tokens = text.lower().split()\n",
    "tokens = word_tokenize(combined_stories.lower())\n",
    "#print(len(tokens))\n",
    "\n",
    "\n",
    "#Creating a word vovabulary based in this tokens\n",
    "special_tokens = [\"<sep>\", \"<unk>\", \"<eos>\"]\n",
    "vocab = list(set(tokens + special_tokens))\n",
    "word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index__word = {idx: word for word, idx in word_index.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(len(word_index))\n",
    "\n",
    "#converting tokens to indices\n",
    "#input_seq = [word_index[word] for word in tokens]\n",
    "#here the list comprehension checks and gets the indexed value for words in the word_index if nor in word_index\n",
    "# assigns the <unk> index to the word\n",
    "input_seq = [word_index.get(word, word_index[\"<unk>\"]) for word in tokens] \n",
    "print(input_seq[:10])\n",
    "print(len(input_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seting device hardware config to train module on either cpu or gpu\n",
    "model = StoryModel(vocab_size, embed_size, hidden_size,num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a function is created to split the tokens into input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_seq, seq_length):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for i in range(len(input_seq) - seq_length):\n",
    "        input_data.append(input_seq[i:i+seq_length])\n",
    "        output_data.append(input_seq[i+seq_length])\n",
    "    return torch.tensor(input_data), torch.tensor(output_data)\n",
    "\n",
    "#Create sequences\n",
    "input_data, output_data = create_sequences(input_seq, seq_length)\n",
    "\n",
    "#just to check the dim of squences\n",
    "print(input_data.ndim)\n",
    "print(output_data.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_data, output_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "print(\"Please Wait\")\n",
    "print(\"Starting Model Training.......\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (batch_input, batch_output) in enumerate(dataloader):\n",
    "        batch_input, batch_output = batch_input.to(device), batch_output.to(device)\n",
    "        hidden = model.init_hidden(batch_input.size(0))  # Initialize hidden state for current batch size\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden = model(batch_input, hidden)\n",
    "\n",
    "        # Use only the final timestep output\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Compute and record loss\n",
    "        loss = criterion(output, batch_output)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss for each epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.2f}')\n",
    "\n",
    "print(\"Model Trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created!!!\n"
     ]
    }
   ],
   "source": [
    "model_filename = 'StoryModel.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "print(\"Model Created!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_story(model, prompt, max_length=100, temperature=1.0):\n",
    "    # Convert the prompt to indices\n",
    "    words = prompt.lower().split()\n",
    "    input_seq = torch.tensor([word_index.get(word, word_index['<unk>']) for word in words], device=device).unsqueeze(0)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Generate words until max length\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        \n",
    "        # Select the last word's output in the sequence\n",
    "        last_word_logits = output[:, -1, :] / temperature\n",
    "        probabilities = torch.softmax(last_word_logits, dim=-1).squeeze()\n",
    "        next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "        next_word = index__word.get(next_word_idx, \"<unk>\")\n",
    "        words.append(next_word)\n",
    "        input_seq = torch.tensor([[next_word_idx]], device=device)\n",
    "        if next_word == \"<sep>\":  # Stop generation at story boundary\n",
    "            break\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define code for cli interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_story():\n",
    "    print(\"########## ''''''''''' ############\")\n",
    "    print(\"Hi am Tobbie a story generator!!!!\")\n",
    "    print(\"########## ''''''''''' ############\")\n",
    "    prompt = input(\"What story would you like to here: \\n\")\n",
    "    while True:\n",
    "        generated = generate_story(model, prompt)\n",
    "        print(f\"\\nLet's begin:\\n\\n{generated}\")\n",
    "        choice = input(\"\\nWhat's next? (Type a new prompt or 'quit' to exit): \").strip()\n",
    "        if choice.lower() == 'quit':\n",
    "            print(\"BYEEE!\")\n",
    "            break\n",
    "        prompt = choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## ''''''''''' ############\n",
      "Hi am Tobbie a story generator!!!!\n",
      "########## ''''''''''' ############\n",
      "\n",
      "Let's begin:\n",
      "\n",
      "alice first , 'it in those of one eye near the time , in very leaves that lay made her mark ] out of my weaknesse , and with eyes like a upon a thousand fire . an old now ? laer . it mocke , when seeing losse within ? guil . aye , but answers it is him then . how long horatio 't ? osr . iudgement man in clouds , go to himselfe . enter hamlet and barnardo . qu . my necessaries you stand ; so sir , is good by my say lady , let\n",
      "BYEEE!\n"
     ]
    }
   ],
   "source": [
    "start_story()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
